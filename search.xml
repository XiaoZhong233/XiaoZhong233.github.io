<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Scrapy爬取CSDN博客列表]]></title>
    <url>%2F2019%2F08%2F16%2FScrapy%E7%88%AC%E5%8F%96CSDN%E5%8D%9A%E5%AE%A2%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 新建Scrapy爬虫项目如果你还没有安装Scrapy，可以通过下面这个命令安装 1pip install scrapy 新建一个项目安装好之后就可以创建项目了 1scrapy startproject 你的项目名 创建好之后的目录如上图所示 每个文件的具体作用可以参照Scrapy的官方文档，这里就不再赘述。 新建一个爬虫通过命令 1scrapy genspider 你的爬虫名 设置配置文件如果你不需要存入数据库或者做进一步的反爬处理，则可以跳过这一步，打开setting.py进行以下修改 设置浏览器名把BOT_NAME设置为你的浏览器名，如果使用默认，别人一看就知道是爬虫 1BOT_NAME = 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11' 设置日志级别这样就不需要打印太多日志信息，干扰视线 1LOG_LEVEL='WARN' 设置模式将机器人模式设为FALSE 1ROBOTSTXT_OBEY = False 设置你要抓取的博客页数12#博客页数CsdnPage = 2; 设置你的数据库为了方便将数据自动导入数据库，需要设置数据库配置信息，我使用的是mysql 1234567#数据库设置MYSQL_HOST = '你的主机名'MYSQL_PORT = 3306 #端口MYSQL_DBNAME = 'DBNAME' # 数据库名MYSQL_TABLE = 'TABLE_NAME' #表名MYSQL_USER = '你的用户名' MYSQL_PASSWD='你的密码' 新建数据项数据项的作用是方便scrapy以结构化的数据插入到数据库中。 打开 items.py，复制一下代码即可，或者你如果要爬新的数据，请记得加入新的字段 12345678import scrapyclass CsdnItem(scrapy.Item): # define the fields for your item here like: title = scrapy.Field() url = scrapy.Field() #date = scrapy.Field() tag = scrapy.Field() pass 编写爬虫文件123456789101112131415161718192021222324252627282930313233import scrapyfrom ..items import CsdnItemfrom ..settings import CsdnPageclass CsdnSpiderSpider(scrapy.Spider): name = &apos;csdn_spider&apos; allowed_domains = [&apos;blog.csdn.net&apos;] start_urls = [&apos;https://blog.csdn.net/weixin_41154636/article/list/1&apos;] def parse(self, response): try: for div in response.xpath(&apos;//div[contains(@class,&quot;article-item-box&quot;)]&apos;): item = CsdnItem() item[&apos;title&apos;] = div.xpath(&apos;./h4/a/text()&apos;)[1].extract().strip() item[&apos;url&apos;] = div.xpath(&apos;./h4/a/@href&apos;)[0].extract().strip() item[&apos;tag&apos;] = item[&apos;url&apos;].split(&apos;/&apos;)[-1] # 爬取时间戳，方便按时间排序 # item[&apos;date&apos;] = div.xpath(&apos;//span[@class=&quot;date&quot;]/text()&apos;)[1].extract().strip() # 我也不知道为什么抓出来会有这个- -，所以特殊处理一下 if item[&apos;title&apos;] == &apos;帝都的凛冬&apos;: continue # 控制台输出 print(item[&apos;title&apos;] + &quot; &quot; + item[&apos;url&apos;] + &quot; &quot; + item[&apos;tag&apos;]) # 封装成bean，装入数据库 yield item # 实现翻页的功能 for page in range(2, CsdnPage + 1): url = &quot;https://blog.csdn.net/weixin_41154636/article/list/%s&quot; % page yield scrapy.Request(url, callback=self.parse) except BaseException: print(BaseException.__cause__) pass 具体代码的说明注释已经写得很清楚了。 另外，需要注意的是，因为CSDN的翻页是通过ajax请求实现的，所以需要自己构造请求。 如果发现跑不起来，很可能是因为CSDN的HTML结构发生了改变，你可以了解一下XPATH的写法，然后修改即可。 先看下运行结果吧： 进入到spiders目录中运行 1scrapy crawl csdn_spider #你的爬虫名，在上面的代码最开始定义的 DONE！信息已经爬下来了，接下来就是把数据保存至数据库中。 保存数据至数据库在setting.py中，把pipeline注释去掉 123ITEM_PIPELINES = &#123; 'csdn.pipelines.CsdnPipeline': 300,&#125; 在piplines.py中编写一下代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pymysqlfrom scrapy.utils.project import get_project_settingsfrom twisted.enterprise import adbapifrom .items import CsdnItemclass DBHelper: def __init__(self): self.settings = get_project_settings() # 获取settings配置数据 dbparams = dict( host=self.settings[&apos;MYSQL_HOST&apos;], # 读取settings中的配置 db=self.settings[&apos;MYSQL_DBNAME&apos;], user=self.settings[&apos;MYSQL_USER&apos;], passwd=self.settings[&apos;MYSQL_PASSWD&apos;], charset=&apos;utf8&apos;, # 编码要加上，否则可能出现中文乱码问题 cursorclass=pymysql.cursors.DictCursor, use_unicode=False, ) # **表示将字典扩展为关键字参数,相当于host=xxx,db=yyy.... dbpool = adbapi.ConnectionPool(&apos;pymysql&apos;, **dbparams) self.dbpool = dbpool def connect(self): return self.dbpool # 插入数据 def insert(self, item): self.settings = get_project_settings() # 获取settings配置数据 if isinstance(item, CsdnItem): tb_name = self.settings[&apos;MYSQL_TABLE&apos;] sql = &quot;&quot;&quot;insert into &quot;&quot;&quot; + tb_name + &quot;&quot;&quot;(id,title,link) values (%s,%s,%s) &quot;&quot;&quot; # print(sql) # 调用插入的方法 query = self.dbpool.runInteraction(self._conditional_insert, sql, item) # 调用异常处理方法 query.addErrback(self._handle_error) return item # 写入数据库中 def _conditional_insert(self, canshu, sql, item): # 取出要存入的数据，这里item就是爬虫代码爬下来存入items内的数据 if isinstance(item, CsdnItem): import datetime # 字符串转为DateTime类型 # dateTime_p = datetime.datetime.strptime(item[&apos;date&apos;], &apos;%Y-%m-%d %H:%M:%S&apos;) params = ( item[&apos;tag&apos;], item[&apos;title&apos;], item[&apos;url&apos;]) canshu.execute(sql, params) # 错误处理方法 def _handle_error(self, failue): pass print(&apos;--------------database operation exception!!-----------------&apos;) # self.connect.rollback() print(failue)# 这里执行scrapy处理脚本class CsdnPipeline(object): def __init__(self): self.db = DBHelper() def process_item(self, item, spider): # 插入数据库 self.db.insert(item) return item 具体说明注释已经写的很清楚了。 这样就完成了导入至数据库的工作 再次运行爬虫。 1scrapy crawl csdn_spider 可以看到，数据就全部自动导入到数据库了！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F08%2F15%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hello world]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理：系统总线总结]]></title>
    <url>%2F2019%2F08%2F14%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%EF%BC%9A%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BA%BF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总线的基本概念历史：早期计算机采用分散连接的方式，这种连接方式以运算器为中心，I/O与存储器交换信息时，都要通过运算器，致使运算器停止运算。为了提高CPU工作效率，改进为存储器为中心的分散连接。随着IO设备的增多，这种连接方式逐渐淘汰，所以出现了总线连接的方式。计算机使用总线结构便于增减外设，同时减少了信息传输线。 知识点： 总线：总线是连接多个部件的信息传输线，是各部件共享的传输介质。以CPU为中心的双总线结构：包括存储总线（M总线）和I/O总线，前者连接CPU和主存，后者连接CPU和I/O。缺点：这种结构在I/O与主存进行交换信息依然要占用CPU单总线结构:单独拉出一条系统总线，CPU，主存，和各IO设备连接到该系统总线。当I/O与主存进行交换信息时，CPU可以继续处理不访问主存或I/O设备的操作。缺点：因为只有一组总线。容器发生冲突，就必须设置判优逻辑，这会影响整机的工作速度以存储器为中心的双总线结构：在单总线的基础上又开辟了一条CPU与主存之间的总线（存储总线）。存储总线只提供主存与CPU直接的信息传输。有点像上面两种结构的折中。 总线的分类分类标准： 按数据传送方式可分为：并行传输总线和串行传输总线 按总线的使用范围可分为 计算机总线，测控总线，网络通信总线 （==重点==）按连接部件分 片内总线，系统总线，通信总线。片内总线：芯片内部的总线，如：CPU内部，寄存器和寄存器之间，寄存器和ALU之间都是由片内总线连接，速度极快。系统总线：系统总线是指：在CPU，主存，I/O设备各大部件之间的信息传输线。系统总线下面按照传输信息的不同又可分为数据总线，地址总线，控制总线1. 数据总线用来传输各部件之间的数据信息，双向数据线。2.地址总线用来指出数据总线上的源数据或目的数据所在主存单元或I/O设备的地址。单向传输：由CPU输出。用来指明CPU要访问的存储单元或I/O地址3.控制总线用来发出各种控制信号的传输线（控制信号，响应信号，时序信号）。对于某个特定的控制线来说他是单向的，但是从总体上看控制总线是双向的。 （举个例子，假如有A,B两条控制线，其中A的方向为CPU到I/O，B的方向为I/O到CPU，对于A,B来说，他们的控制信号传输方向是不能改变的，但从总体上看，控制总线既有从CPU-&gt;I/O的，也有I/O到CPU的，所以总体上是双向的） 通信总线按传输方式分为两种：串行通信和并行通信 1.串行通信串行通信是指单条1位宽的数据线，一位一位分时地进行传输。 2.并行通信多条并行的1位宽的数据，同时进行传输。 3.对比 - 串行通信 并行通信 优点 稳定，信号不易受到干扰 传输速率快 缺点 传输速率慢 易受干扰 适用场景 远距离传输 近距离传输 # 总线的特性及性能指标 ## 总线特性 1. 机械特性 指连接方式上的一些特性 2. 电气特性 指每个传输线的信号的传递方向和电平范围（什么范围为高？什么范围为低） 3. 功能特性 每个传输线的功能 4. 时间特性 任一根线在什么时间内有效 总线指标 总线宽度：数据总线的根数，用位（bit）表示 总线带宽：单位时间内总线上传输数据的位数 通常用MBps（兆字节每秒） （注意与bps进行区别，bps指的是波特率，是单位时间内传输的位数） 总线复用：一条信号线上分时传送两种信号。采用多路复用技术实现 时钟同步/异步 信号线数 总线控制方式：包括突发工作、自动配置、仲裁方式、逻辑方式、计数方式等 其他指标：负载能力、电源电压、总线宽度是否可以扩展等 负载能力：指驱动能力，当总线接上负载后，总线输入输出的逻辑电平是否保持在额定范围内。 总线标准1. ISA总线 具有独立于CPU的总线时钟 总线宽度为16位，地址线为24位 CPU需花大量时间来控制和外部设备交换数据2.EISA总线 在ISA总线的基础得来，从CPU分离出了总线控制权 总线宽度为32位，地址宽度为32位3.VESA(VL-BUS)总线 总线宽度为32位，可扩展至64位4.PCI总线 高性能，不依赖某个具体的处理器，数据线为32位可扩展至64位 兼容性良好 支持即插即用（Plug and Play）（PCI和USB都支持） 采用多路复用技术 ………………. 5.AGP总线AGP（Accelerated Graphics Port 加速图形端口）处理三维数据和图形的总线，一般用于显卡。 6.RS-232C总线一种串行通信总线，可用于实现载波通信 7.USB总线通用串行总线（USB）具有以下特征 即插即用 很强的连接能力，可以连接多个外设到一个系统 数据传输率 1.0可达1.5Mbps 2.0可达480Mbps 标准统一 体积小巧 生命力强 总线结构一般分为单总线结构和多总线结构 单总线结构有个系统总线，CPU，主存，I/O设备都挂在上面，容易造成计算机性能的瓶颈。 多总线结构双总线结构：包括主存总线和I/O总线三总线结构：包括主存总线（CPU和主存之间）、I/O总线（CPU和IO之间）和DMA总线（主存和IO之间） 总线控制总线控制包括了判优控制和通信控制：主模块：主模块对总线有控制权从模块：从模块只能响应主模块发来的总线命令，没有总线控制权 判优控制判优控制可分为集中式和分布式两种 集中式 链式查询其控制总线由BS（总线忙）,BR（总线请求）,BG（总线同意）三条线构成。BG是串行地由一个I/O接口送到下一个I/O接口。如果到达的接口有请求，BG信号就不再往下传，意味着该接口获得了总线的使用权，然后建立BS（总线忙）信号。优先级逐级递减，对电路故障敏感 计数器定时查询相比链式查询，少了一根总线同意线（BG），多了一组设备地址线。工作方式： 总线控制器接到BR线送来的总线请求信号后，在总线未被使用的情况下（BS=0）由计数器开始计数，并通过设备地址线发出一组地址信号，然后某个与该信号一致的设备获得总线使用权。此时终止计数查询，查询可以从0开始（此时优先级逐级递减），也可以从上一次终止计数的地方开始（此时各部件优先级相同，这种也叫循环计数） 独立请求方式独立请求方式，每一设备都有单独的一组BR（总线请求）和BG（总线同意）。而总线控制部件中有一排队电路，可按优先次序确定响应哪个设备的请求。特点是响应时间短，速度快，但控制复杂 对比 - 优点 缺点 控制线数 链式查询 控制简单，仅用三根线即可控制并且很容易扩充设备 对电路故障敏感，低优先级的设备很难获得控制权 3根 计数器定时查询 优先次序可以被改变，对电路故障没那么敏感 增加了控制线，控制也比较复杂 ${log_2{n}}$ 独立请求方式 响应速度快，优先次序控制灵活 控制线线路较多，控制复杂 2n 分布式考纲和书上没有，不总结 通信控制目的：解决通信双方如何获知信息传输开始和传输结束，以及通信双方如何协调与配合可分为：同步，异步，半同步，分离式 总线周期（通信周期） 申请分配阶段由主模块提出申请，经过总线仲裁机构决定下一周期总线使用权给哪个申请者 寻址阶段获得了使用权的主模块通过总线发出本次要访问的从模块的地址和有关命令 传数阶段主模块和从模块进行数据交换。 结束阶段主模块让出总线控制权 通信方式同步通信通信双方由统一时钟信号控制下进行通信，时钟信号通常由CPU的总线控制部件发出，每个周期内完成特定的任务。 异步通信不采用统一的时钟信号控制通信，而采用应答的方式进行通信可分为：不互锁，半互锁，全互锁 1. 不互锁 对于主模块来说：主模块发出信息后，不需要等待街道从模块的回答信号，而是经过一段时间，便撤销请求信号 对于从模块来说：从模块接收到请求后，在条件就发出回答信号，并经过一段时间，便撤销回答信号。 应用场景：CPU向主存写信息 2.半互锁 对于主模块来说：主模块发出请求信号，必须接到从模块的回答信号才撤销其请求信号，有互锁关系 对于从模块来说：从模块接到请求信号后发出回答信号，而不必等待获知主模块的撤销请求信号，而是隔一段时间后就撤回回答信号，无互锁关系。 应用场景多机系统中，CPU访问共享存储器，CPU发出访存命令后，必须接收到存储器未被占用的回答信号，才能进行真正的访存操作 3.全互锁 对于主模块来说：主模块发出请求信号，必须接收到从模块的回答信号，然后发出撤销请求信号，存在互锁关系 对于从模块来说：从模块发出回答信号之后，必须等待获知主模块的撤销请求信号，存在互锁关系 应用场景：网络通信，例如著名的三次握手半同步通信半同步通信既保留了同步通信的特点（发送方所有的地址，命令、数据信号的发出时间在系统时钟的某个上升沿开始，接收方都采用系统时钟后沿时刻进行识别判断），同时又通过插入N个“等待”（WAIT）信号解决与协调通信速度不一致的问题，双方像异步通信那也，允许不同速度的模块进行工作分离式通信进一步分析总线传输周期得知，除了申请总线这一阶段，其余时间主要花在下面3个地方 主模块通过传输总线向从模块发出地址与命令 从模块准备数据 从模块经过数据总线向主模块发送数据 可以看见，在2中从模块准备数据的过程中，总线并没有完全利用，处于等待状态，为了发掘系统每一瞬间的潜力，因此采用了分离式通信。分离式通信将传输周期（or总线周期）分为两个子周期。 第一个子周期模块A在获得总线使用权后，将命令，地址等信息发到系统总线上，然后立刻放弃总线使用权，给别的模块使用 第二个子周期B模块接收到命令后，经过一段时间（译码，读取）准备好数据之后，申请总线控制权，一旦获批，B模块便把数据放到系统总线上，传输给A，然后立刻放弃总线控制权。显然：上面两个传输子周期都只有单方向的信息流，每个模块都变成了主模块。]]></content>
      <categories>
        <category>计算机理论</category>
      </categories>
      <tags>
        <tag>计组</tag>
      </tags>
  </entry>
</search>
